# Uncanny Valley VR Project

## Overview
The **Uncanny Valley VR Project** explores the uncanny valley effect in virtual reality by analyzing and balancing variables such as lip sync accuracy, eye motion realism, and facial tracking precision. The project aims to improve user perception and engagement with virtual avatars.

## Features
- **Facial Tracking Accuracy**: Ensures the avatar's facial expressions closely match the user's.
- **Lip Sync Implementation**: Uses real-time facial tracking instead of audio-based lip sync.
- **Eye Subtle Motion**: Random blinking and eye movements for enhanced realism.
- **Noise and Imperfections**: Studies the impact of minor inaccuracies on user perception.
- **Hand Tracking Interactions**: Allows users to interact naturally using gestures.
- **Experiment Design**: Provides instructions guiding users through interactions to evaluate uncanny valley triggers.

## Research Goals
- Understand how tracking accuracy affects the uncanny valley effect.
- Improve realism in avatar facial animation using FACS (Facial Action Coding System).
- Study the impact of subtle eye motion and lip sync variations.
- Develop a framework for measuring user perception in VR interactions.

## Tech Stack
- **Platform**: Meta Quest Pro
- **Game Engine**: Unity with Oculus Integration SDK
- **Languages**: C#
- **Libraries & Tools**:
  - Meta Movement SDK (Body & Facial Tracking)
  - OpenXR & Oculus SDK
  - Unity's Animator for facial animations

## Installation & Setup
1. Clone the repository:
   ```sh
   git clone https://github.com/THANKSHANK/UncannyValleyVRProject.git
   cd uncanny-valley-vr
   ```
2. Open the project in Unity.
3. Install the required Oculus and OpenXR packages.
4. Connect Meta Quest Pro and enable facial tracking.
5. Run the project in VR mode.

## Usage
- Start the VR experience and follow the guided interactions.
- Observe how facial tracking and eye movements affect perception.
- Provide feedback through built-in survey tools.

## Future Improvements
- Enhanced AI-driven animation blending.
- More detailed facial expressions based on user feedback.
- Expanded dataset for analyzing the uncanny valley effect.
- Integration with additional VR headsets.

## Contributors
- **Zihan Wang** - Project Lead & Developer

  
## Acknowledgments
- Special thanks to research advisors and contributors.
- Inspired by FACS, AI-driven animation, and VR interaction studies.

